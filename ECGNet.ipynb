{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import ast\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler,normalize, MinMaxScaler\n",
    "from scipy.signal import spectrogram, resample\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from wandb.keras import WandbCallback\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score,f1_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aggregate-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing \n",
    "#   Using the super classes, multi label classification, excluding samples with no labels and considering atleast one label\n",
    "\n",
    "path = 'C:/ptb/'\n",
    "Y = pd.read_csv(path+ 'ptbxl_database.csv', index_col = 'ecg_id')\n",
    "\n",
    "\n",
    "\n",
    "data = np.array([wfdb.rdsamp(path+f)[0] for f in Y.filename_lr])\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "agg_df = pd.read_csv(path+ 'scp_statements.csv', index_col = 0)\n",
    "\n",
    "agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "\n",
    "def agg(y_dic):\n",
    "    temp =[]\n",
    "    \n",
    "    for key in y_dic.keys():\n",
    "        if key in agg_df.index:\n",
    "            c = agg_df.loc[key].diagnostic_class\n",
    "            if str(c) != 'nan':\n",
    "                temp.append(c)\n",
    "    return list(set(temp))\n",
    "\n",
    "Y['diagnostic_superclass'] = Y.scp_codes.apply(agg)\n",
    "Y['superdiagnostic_len'] = Y['diagnostic_superclass'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "#########\n",
    "\n",
    "counts = pd.Series(np.concatenate(Y.diagnostic_superclass.values)).value_counts()\n",
    "\n",
    "Y['diagnostic_superclass'] = Y['diagnostic_superclass'].apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "\n",
    "X_data = data[Y['superdiagnostic_len'] >= 1]\n",
    "Y_data = Y[Y['superdiagnostic_len'] >= 1]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(Y_data['diagnostic_superclass'])\n",
    "y = mlb.transform(Y_data['diagnostic_superclass'].values)\n",
    "\n",
    "########\n",
    "\n",
    "## Stratify split\n",
    "\n",
    "X_train = X_data[Y_data.strat_fold < 9]\n",
    "y_train = y[Y_data.strat_fold < 9]\n",
    "\n",
    "X_val = X_data[Y_data.strat_fold == 9]\n",
    "y_val = y[Y_data.strat_fold == 9]\n",
    "\n",
    "X_test = X_data[Y_data.strat_fold == 10]\n",
    "y_test = y[Y_data.strat_fold == 10]\n",
    "\n",
    "del X_data, Y_data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yellow-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "\n",
    "def apply_scaler(X, scaler):\n",
    "    X_tmp = []\n",
    "    for x in X:\n",
    "        x_shape = x.shape\n",
    "        X_tmp.append(scaler.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n",
    "    X_tmp = np.array(X_tmp)\n",
    "    return X_tmp\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n",
    "\n",
    "X_train_scale = apply_scaler(X_train, scaler)\n",
    "X_test_scale = apply_scaler(X_test, scaler)\n",
    "X_val_scale = apply_scaler(X_val, scaler)\n",
    "\n",
    "del X_train, X_test, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conventional-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(Dataset):\n",
    "    def __init__(self, X, y,batch_size = 16):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.X) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        batch_x = self.X[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
    "        \n",
    "        return torch.tensor(batch_x, dtype = torch.float32), torch.tensor(batch_y, dtype = torch.float32)\n",
    "    \n",
    "## Params\n",
    "\n",
    "batch_size = 16\n",
    "    \n",
    "train_gen = DataGen(X_train_scale, y_train, batch_size = batch_size)\n",
    "test_gen = DataGen(X_test_scale, y_test, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungarian-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=in_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=out_channels)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
    "                               stride=stride, padding=padding, bias=False)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.downsample = downsample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            out = self.maxpool(out)\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ECGNet(nn.Module):\n",
    "\n",
    "    def __init__(self, struct=[15, 17, 19, 21], in_channels=12, fixed_kernel_size=17, num_classes=5):\n",
    "        super(ECGNet, self).__init__()\n",
    "        self.struct = struct\n",
    "        self.planes = 16\n",
    "        self.parallel_conv = nn.ModuleList()\n",
    "\n",
    "        for i, kernel_size in enumerate(struct):\n",
    "            sep_conv = nn.Conv1d(in_channels=in_channels, out_channels=self.planes, kernel_size=kernel_size,\n",
    "                               stride=1, padding=0, bias=False)\n",
    "            self.parallel_conv.append(sep_conv)\n",
    "        # self.parallel_conv.append(nn.Sequential(\n",
    "        #     nn.MaxPool1d(kernel_size=2, stride=2, padding=0),\n",
    "        #     nn.Conv1d(in_channels=1, out_channels=self.planes, kernel_size=1,\n",
    "        #                        stride=1, padding=0, bias=False)\n",
    "        # ))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.planes, out_channels=self.planes, kernel_size=fixed_kernel_size,\n",
    "                               stride=2, padding=2, bias=False)\n",
    "        self.block = self._make_layer(kernel_size=fixed_kernel_size, stride=1, padding=8)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=self.planes)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=8, stride=8, padding=2)\n",
    "        self.rnn = nn.LSTM(input_size=12, hidden_size=40, num_layers=1, bidirectional=False)\n",
    "        self.fc = nn.Linear(in_features=168, out_features=num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, kernel_size, stride, blocks=15, padding=0):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        base_width = self.planes\n",
    "\n",
    "        for i in range(blocks):\n",
    "            if (i + 1) % 4 == 0:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv1d(in_channels=self.planes, out_channels=self.planes + base_width, kernel_size=1,\n",
    "                               stride=1, padding=0, bias=False),\n",
    "                    nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "                )\n",
    "                layers.append(ResBlock(in_channels=self.planes, out_channels=self.planes + base_width, kernel_size=kernel_size,\n",
    "                                       stride=stride, padding=padding, downsample=downsample))\n",
    "                self.planes += base_width\n",
    "            elif (i + 1) % 2 == 0:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "                )\n",
    "                layers.append(ResBlock(in_channels=self.planes, out_channels=self.planes, kernel_size=kernel_size,\n",
    "                                       stride=stride, padding=padding, downsample=downsample))\n",
    "            else:\n",
    "                downsample = None\n",
    "                layers.append(ResBlock(in_channels=self.planes, out_channels=self.planes, kernel_size=kernel_size,\n",
    "                                       stride=stride, padding=padding, downsample=downsample))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_sep = []\n",
    "\n",
    "        for i in range(len(self.struct)):\n",
    "            sep = self.parallel_conv[i](x)\n",
    "            out_sep.append(sep)\n",
    "\n",
    "        out = torch.cat(out_sep, dim=2)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)  # out => [b, 16, 9960]\n",
    "\n",
    "        out = self.block(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.avgpool(out)  # out => [b, 64, 10]\n",
    "        out = out.reshape(out.shape[0], -1)  # out => [b, 640]\n",
    "\n",
    "        rnn_out, (rnn_h, rnn_c) = self.rnn(x.permute(2, 0, 1))\n",
    "        new_rnn_h = rnn_h[-1, :, :]  # rnn_h => [b, 40]\n",
    "\n",
    "        new_out = torch.cat([out, new_rnn_h], dim=1)  # out => [b, 680]\n",
    "        \n",
    "        result = self.fc(new_out)  # out => [b, 20]\n",
    "\n",
    "        # print(out.shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "model = ECGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrong-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('ECGNet_saves'):\n",
    "    os.mkdir('ECGNet_saves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proper-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlikith012\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ECGNet</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/likith012/BaseECG\" target=\"_blank\">https://wandb.ai/likith012/BaseECG</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/likith012/BaseECG/runs/398by3q1\" target=\"_blank\">https://wandb.ai/likith012/BaseECG/runs/398by3q1</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20210317_232807-398by3q1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(398by3q1)</h1><p></p><iframe src=\"https://wandb.ai/likith012/BaseECG/runs/398by3q1\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe5d506b700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def metrics(y_true, y_scores):\n",
    "    y_pred = y_scores >= 0.5\n",
    "    acc = np.zeros(y_pred.shape[-1])\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_true, y_scores, average = 'macro')\n",
    "    \n",
    "    for i in range(y_pred.shape[-1]):\n",
    "        acc[i] = accuracy_score(y_true[:,i], y_pred[:,i])\n",
    "    return acc, np.mean(acc), roc_auc\n",
    "\n",
    "wandb.init(project = 'BaseECG', name = 'ECGNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "geographic-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_func, dataset, epoch):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    pred_all = []\n",
    "    loss_all = []\n",
    "    gt_all = []\n",
    "    \n",
    "    for batch_step in tqdm(range(len(dataset)) , desc=\"train\"):\n",
    "        batch_x, batch_y = dataset[batch_step]    \n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_x = batch_x.permute(0,2,1)\n",
    "        batch_y = batch_y.cuda()\n",
    "\n",
    "        pred = model(batch_x)\n",
    "        pred_all.append(pred.cpu().detach().numpy())\n",
    "        \n",
    "        loss = loss_func(pred, batch_y)\n",
    "        loss_all.append(loss.cpu().detach().item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        gt_all.extend(batch_y.cpu().detach().numpy())\n",
    "\n",
    "    print('epoch {0} '.format(epoch))\n",
    "    print('train_loss ', np.mean(loss_all))\n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "\n",
    "    _, mean_acc, roc_score = metrics(np.array(gt_all), pred_all )\n",
    "    wandb.log({'train_mean_accuracy' : mean_acc, 'epoch':epoch})\n",
    "    wandb.log({'train_roc_score' : roc_score, 'epoch':epoch})\n",
    "    wandb.log({'train_loss' : np.mean(loss_all) , 'epoch':epoch})\n",
    "\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model, loss_func, dataset):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    pred_all = []\n",
    "    loss_all = []\n",
    "    gt_all = []\n",
    "    \n",
    "    for batch_step in tqdm(range(len(dataset)) , desc=\"test\"):\n",
    "        batch_x, batch_y = dataset[batch_step]\n",
    "        batch_x = batch_x.cuda()\n",
    "        batch_x = batch_x.permute(0,2,1)\n",
    "        batch_y = batch_y.cuda()\n",
    "        \n",
    "        pred = model(batch_x)\n",
    "        pred_all.append(pred.cpu().detach().numpy())\n",
    "       \n",
    "        loss = loss_func(pred, batch_y)\n",
    "        loss_all.append(loss.cpu().detach().numpy())\n",
    "        gt_all.extend(batch_y.cpu().detach().numpy())\n",
    "\n",
    "    print('test_loss ', np.mean(loss_all))\n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "\n",
    "    _, mean_acc, roc_score = metrics(np.array(gt_all), pred_all )\n",
    "    wandb.log({'test_mean_accuracy' : mean_acc, 'epoch':epoch})\n",
    "    wandb.log({'test_roc_score' : roc_score, 'epoch':epoch})\n",
    "    wandb.log({'test_loss' : np.mean(loss_all) , 'epoch':epoch})\n",
    "\n",
    "    return np.mean(loss_all), mean_acc, roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [02:02<00:00,  8.72it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 37.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \n",
      "train_loss  0.36884335230722604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.85it/s]\n",
      "train:   0%|          | 1/1070 [00:00<01:50,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.359931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [02:03<00:00,  8.65it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 27.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \n",
      "train_loss  0.31217912286520005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 34.86it/s]\n",
      "train:   0%|          | 1/1070 [00:00<01:53,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.33083642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [02:04<00:00,  8.62it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 29.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 \n",
      "train_loss  0.29330473386934985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.61it/s]\n",
      "train:   0%|          | 1/1070 [00:00<01:50,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31379893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [02:03<00:00,  8.68it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 29.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 \n",
      "train_loss  0.28176224891827484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 37.76it/s]\n",
      "train:   0%|          | 1/1070 [00:00<01:49,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31501555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:53<00:00,  9.40it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 \n",
      "train_loss  0.27314382198954296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.87it/s]\n",
      "train:   0%|          | 2/1070 [00:00<01:21, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31343472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:20<00:00, 13.37it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 40.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 \n",
      "train_loss  0.2647550921330107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.75it/s]\n",
      "train:   0%|          | 2/1070 [00:00<01:19, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.3132583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:23<00:00, 12.75it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 \n",
      "train_loss  0.2579409536566133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.30885902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:23<00:00, 12.89it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 40.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 \n",
      "train_loss  0.2504917144287969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31293997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:35<00:00, 11.23it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 38.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 \n",
      "train_loss  0.2443331797615947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31405863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:27<00:00, 12.19it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 \n",
      "train_loss  0.23834283063275236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.31960666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:33<00:00, 11.41it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 \n",
      "train_loss  0.23136993729135144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.3308096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:31<00:00, 11.69it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 38.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 \n",
      "train_loss  0.2244844285719027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.33628672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:32<00:00, 11.53it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 \n",
      "train_loss  0.21883703653013037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.347952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:26<00:00, 12.43it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 \n",
      "train_loss  0.21207367255587445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.3545371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:26<00:00, 12.33it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 39.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 \n",
      "train_loss  0.2033620270056145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.368325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:26<00:00, 12.31it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 26.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 \n",
      "train_loss  0.19422555699178548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 35.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.3670307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:27<00:00, 12.22it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 39.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 \n",
      "train_loss  0.18699470627008477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 35.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.3813329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:35<00:00, 11.21it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 \n",
      "train_loss  0.17815066495579537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.39931965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:29<00:00, 11.91it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 29.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 \n",
      "train_loss  0.17024776575582048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 39.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.41593003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:31<00:00, 11.71it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 \n",
      "train_loss  0.16174233258815013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.4332475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:25<00:00, 12.52it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 \n",
      "train_loss  0.15276617591974334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.4364877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:32<00:00, 11.60it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 42.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 \n",
      "train_loss  0.1465741006013389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.45080858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:19<00:00, 13.38it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 \n",
      "train_loss  0.13748424958402866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5201003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:32<00:00, 11.54it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 \n",
      "train_loss  0.1327014625995098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5026201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:24<00:00, 12.68it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 \n",
      "train_loss  0.1259953607517843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.50263786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:30<00:00, 11.84it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 \n",
      "train_loss  0.1195528215058496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.54139996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:25<00:00, 12.56it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26 \n",
      "train_loss  0.11388346465113007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5564948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:21<00:00, 13.21it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27 \n",
      "train_loss  0.11041517015372482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5459528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:30<00:00, 11.82it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 38.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 \n",
      "train_loss  0.1024366324140786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5629619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:30<00:00, 11.88it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29 \n",
      "train_loss  0.0983938837524016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.60391575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:29<00:00, 12.01it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 \n",
      "train_loss  0.09616881861421062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.5599215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:28<00:00, 12.14it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31 \n",
      "train_loss  0.08973731704700355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.6520616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:29<00:00, 12.01it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 \n",
      "train_loss  0.08482296666147832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.62545764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:34<00:00, 11.27it/s]\n",
      "test:   2%|▏         | 3/136 [00:00<00:04, 27.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33 \n",
      "train_loss  0.08203077191338988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.650081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:21<00:00, 13.06it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 \n",
      "train_loss  0.07539276468054017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.6485173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:28<00:00, 12.08it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 42.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35 \n",
      "train_loss  0.07368728988934482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.6730396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:23<00:00, 12.83it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36 \n",
      "train_loss  0.07017894171865953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.7095008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:26<00:00, 12.31it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 \n",
      "train_loss  0.06741760024043653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.66834205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:24<00:00, 12.71it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 \n",
      "train_loss  0.06698919117938135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.702291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:30<00:00, 11.86it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39 \n",
      "train_loss  0.059254360105407154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 41.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.6618781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:28<00:00, 12.07it/s]\n",
      "test:   4%|▎         | 5/136 [00:00<00:03, 41.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 \n",
      "train_loss  0.0597133028340552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.72156996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1070/1070 [01:24<00:00, 12.65it/s]\n",
      "test:   3%|▎         | 4/136 [00:00<00:03, 39.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41 \n",
      "train_loss  0.057202074084121944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 136/136 [00:03<00:00, 40.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss  0.7991146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  52%|█████▏    | 558/1070 [00:45<00:40, 12.77it/s]"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 60\n",
    "\n",
    "model.cuda()\n",
    "# wandb.watch(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_step = train_model(model, optimizer, loss_func, train_gen, epoch)\n",
    "    test_step = test_model(model, loss_func, test_gen)\n",
    "\n",
    "    if epoch > 5 :\n",
    "        torch.save(model.state_dict(), f'ECGNet_saves/{epoch}__{test_step[2]:.4f}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "outside-ordinance",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ECGNet(\n",
       "  (parallel_conv): ModuleList(\n",
       "    (0): Conv1d(12, 16, kernel_size=(15,), stride=(1,), bias=False)\n",
       "    (1): Conv1d(12, 16, kernel_size=(17,), stride=(1,), bias=False)\n",
       "    (2): Conv1d(12, 16, kernel_size=(19,), stride=(1,), bias=False)\n",
       "    (3): Conv1d(12, 16, kernel_size=(21,), stride=(1,), bias=False)\n",
       "  )\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (conv1): Conv1d(16, 16, kernel_size=(17,), stride=(2,), padding=(2,), bias=False)\n",
       "  (block): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(16, 16, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(16, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(32, 32, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(32, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(32, 48, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (9): ResBlock(\n",
       "      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ResBlock(\n",
       "      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(48, 48, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (11): ResBlock(\n",
       "      (bn1): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(48, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(48, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResBlock(\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (13): ResBlock(\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (downsample): Sequential(\n",
       "        (0): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResBlock(\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(17,), stride=(1,), padding=(8,), bias=False)\n",
       "      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool): AvgPool1d(kernel_size=(8,), stride=(8,), padding=(2,))\n",
       "  (rnn): LSTM(12, 40)\n",
       "  (fc): Linear(in_features=168, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "PATH = r'C:\\Users\\likit\\OneDrive\\Desktop\\Cardio-Viz\\Code\\BaseECG\\ECGNet\\11__0.9102.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "related-diagnosis",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9101563184785769"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_gen = DataGen(X_test_scale, y_test, batch_size=len(X_test_scale))\n",
    "\n",
    "batch = test_gen[0][0].permute(0,2,1)\n",
    "pred = model(batch).detach()\n",
    "\n",
    "roc_auc_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ancient-technical",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "roc_score : 0.9101563184785769\nclass wise AUC : [0.91022348 0.88426456 0.9021273  0.93360477 0.92056149]\n"
     ]
    }
   ],
   "source": [
    "### Class wise AUC\n",
    "\n",
    "roc_score = roc_auc_score(y_test, pred, average='macro')\n",
    "print(f'roc_score : {roc_score}')\n",
    "\n",
    "def AUC(y_true: np.ndarray, y_pred: np.ndarray, verbose=False) -> float:\n",
    "    \"\"\"Computes the macro-average AUC score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): list of labels\n",
    "        y_pred (np.ndarray): list of predicted probabilities\n",
    "\n",
    "    Returns:\n",
    "        float: macro-average AUC score.\n",
    "    \"\"\"\n",
    "    aucs = []\n",
    "    assert len(y_true.shape) == 2 and len(y_pred.shape) == 2, 'Predictions and labels must be 2D.'\n",
    "    for col in range(y_true.shape[1]):\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_true[:, col], y_pred[:, col]))\n",
    "        except ValueError as e:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f'Value error encountered for label {col}, likely due to using mixup or '\n",
    "                    f'lack of full label presence. Setting AUC to accuracy. '\n",
    "                    f'Original error was: {str(e)}.'\n",
    "                )\n",
    "            aucs.append((y_pred == y_true).sum() / len(y_pred))\n",
    "    return np.array(aucs)\n",
    "\n",
    "class_auc = AUC(y_test, pred)\n",
    "print(f'class wise AUC : {class_auc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "established-appointment",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "class wise accuracy: [0.88719371 0.90661119 0.84604716 0.85899214 0.86870088]\naccuracy: 0.8735090152565881\n"
     ]
    }
   ],
   "source": [
    "### Accuracy metric\n",
    "\n",
    "def metrics(y_true, y_scores):\n",
    "    y_pred = y_scores >= 0.5\n",
    "    acc = np.zeros(y_pred.shape[-1])\n",
    "    \n",
    "    for i in range(y_pred.shape[-1]):\n",
    "        acc[i] = accuracy_score(y_true[:,i], y_pred[:,i])\n",
    "    return acc, np.mean(acc)\n",
    "\n",
    "acc, mean_acc = metrics(y_test, pred)\n",
    "print(f'class wise accuracy: {acc}')\n",
    "print(f'accuracy: {mean_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "narrow-madison",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CD       0.86      0.61      0.71       498\n",
      "         HYP       0.77      0.33      0.47       263\n",
      "          MI       0.80      0.53      0.64       553\n",
      "        NORM       0.79      0.92      0.85       964\n",
      "        STTC       0.83      0.58      0.68       523\n",
      "\n",
      "   micro avg       0.81      0.67      0.73      2801\n",
      "   macro avg       0.81      0.59      0.67      2801\n",
      "weighted avg       0.81      0.67      0.72      2801\n",
      " samples avg       0.75      0.70      0.71      2801\n",
      "\n",
      "C:\\Users\\likit\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_values = pred >= 0.5\n",
    "\n",
    "report = classification_report(y_test, pred_values, target_names = mlb.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lesser-canvas",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7712464813278368,\n",
       " 0.9101563184785769,\n",
       " array([0.77124648, 0.76814814, 0.76475761, 0.76157367, 0.75672646,\n",
       "        0.75207953, 0.74914798, 0.74659073, 0.7404506 , 0.73561017]),\n",
       " array([0.7926559 , 0.79706679, 0.8023798 , 0.80757946, 0.81168563,\n",
       "        0.81620603, 0.82248596, 0.82982319, 0.83320099, 0.8367813 ]),\n",
       " array([0.75096317, 0.74125443, 0.73050547, 0.72052705, 0.70873786,\n",
       "        0.69729542, 0.68781785, 0.6785329 , 0.6662814 , 0.65626445]),\n",
       " array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "        0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ]))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "def multi_threshold_precision_recall(y_true: np.ndarray, y_pred: np.ndarray, thresholds: np.ndarray) :\n",
    "    \n",
    "    # Expand analysis to number of thresholds\n",
    "    y_pred_bin = np.repeat(y_pred[None, :, :], len(thresholds), axis=0) >= thresholds[:, None, None]\n",
    "    y_true_bin = np.repeat(y_true[None, :, :], len(thresholds), axis=0)\n",
    "\n",
    "    # Compute true positives\n",
    "    TP = np.sum(np.logical_and(y_true, y_pred_bin), axis=2)\n",
    "\n",
    "    # Compute macro-average precision handling all warnings\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        den = np.sum(y_pred_bin, axis=2)\n",
    "        precision = TP / den\n",
    "        precision[den == 0] = np.nan\n",
    "        with warnings.catch_warnings():  # for nan slices\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            av_precision = np.nanmean(precision, axis=1)\n",
    "\n",
    "    # Compute macro-average recall\n",
    "    recall = TP / np.sum(y_true_bin, axis=2)\n",
    "    av_recall = np.mean(recall, axis=1)\n",
    "\n",
    "    return av_precision, av_recall\n",
    "\n",
    "\n",
    "def metric_summary(y_true: np.ndarray, y_pred: np.ndarray, num_thresholds: int = 10) :\n",
    "    \n",
    "    thresholds = np.arange(0.00, 1.01, 1. / (num_thresholds - 1), float)\n",
    "    average_precisions, average_recalls = multi_threshold_precision_recall(\n",
    "        y_true, y_pred, thresholds\n",
    "    )\n",
    "    f_scores = 2 * (average_precisions * average_recalls) / (average_precisions + average_recalls)\n",
    "    auc = np.array(AUC(y_true, y_pred, verbose=True)).mean()\n",
    "    return (\n",
    "        f_scores[np.nanargmax(f_scores)],\n",
    "        auc,\n",
    "        f_scores,\n",
    "        average_precisions,\n",
    "        average_recalls,\n",
    "        thresholds\n",
    "    )\n",
    "\n",
    "metric_summary(y_test, pred.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}